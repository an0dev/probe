---
title: Huggingface
---

To use Probe with Huggingface models, set the `model` flag:

<CodeGroup>

```bash Terminal
probe --model huggingface/<huggingface-model>
```

```python Python
from probe import probe

probe.llm.model = "huggingface/<huggingface-model>"
probe.chat()
```

</CodeGroup>

You may also need to specify your Huggingface api base url:
<CodeGroup>

```bash Terminal
probe --api_base <https://my-endpoint.huggingface.cloud>
```

```python Python
from probe import probe

probe.llm.api_base = "https://my-endpoint.huggingface.cloud"
probe.chat()
```

</CodeGroup>

# Supported Models

Probe should work with almost any text based hugging face model.

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable   | Description                 | Where to Find                                                                      |
| ---------------------- | --------------------------- | ---------------------------------------------------------------------------------- |
| `HUGGINGFACE_API_KEY` | Huggingface account API key | [Huggingface -> Settings -> Access Tokens](https://huggingface.co/settings/tokens) |
