---
title: Anyscale
---

To use Probe with a model from Anyscale, set the `model` flag:

<CodeGroup>

```bash Terminal
probe --model anyscale/<model-name>
```

```python Python
from probe import probe

# Set the model to use from Anyscale:
probe.llm.model = "anyscale/<model-name>"
probe.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from Anyscale:

- Llama 2 7B Chat
- Llama 2 13B Chat
- Llama 2 70B Chat
- Mistral 7B Instruct
- CodeLlama 34b Instruct

<CodeGroup>

```bash Terminal
probe --model anyscale/meta-llama/Llama-2-7b-chat-hf
probe --model anyscale/meta-llama/Llama-2-13b-chat-hf
probe --model anyscale/meta-llama/Llama-2-70b-chat-hf
probe --model anyscale/mistralai/Mistral-7B-Instruct-v0.1
probe --model anyscale/codellama/CodeLlama-34b-Instruct-hf
```

```python Python
probe.llm.model = "anyscale/meta-llama/Llama-2-7b-chat-hf"
probe.llm.model = "anyscale/meta-llama/Llama-2-13b-chat-hf"
probe.llm.model = "anyscale/meta-llama/Llama-2-70b-chat-hf"
probe.llm.model = "anyscale/mistralai/Mistral-7B-Instruct-v0.1"
probe.llm.model = "anyscale/codellama/CodeLlama-34b-Instruct-hf"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                            | Where to Find                                                               |
| -------------------- | -------------------------------------- | --------------------------------------------------------------------------- |
| `ANYSCALE_API_KEY`   | The API key for your Anyscale account. | [Anyscale Account Settings](https://app.endpoints.anyscale.com/credentials) |
