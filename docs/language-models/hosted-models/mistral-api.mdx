---
title: Mistral AI API
---

To use Probe with the Mistral API, set the `model` flag:

<CodeGroup>

```bash Terminal
probe --model mistral/<mistral-model>
```

```python Python
from probe import probe

probe.llm.model = "mistral/<mistral-model>"
probe.chat()
```

</CodeGroup>

# Supported Models

We support the following completion models from the Mistral API:

- mistral-tiny
- mistral-small
- mistral-medium

<CodeGroup>

```bash Terminal
probe --model mistral/mistral-tiny
probe --model mistral/mistral-small
probe --model mistral/mistral-medium
```

```python Python
probe.llm.model = "mistral/mistral-tiny"
probe.llm.model = "mistral/mistral-small"
probe.llm.model = "mistral/mistral-medium"
```

</CodeGroup>

# Required Environment Variables

Set the following environment variables [(click here to learn how)](https://chat.openai.com/share/1062cdd8-62a1-4aa8-8ec9-eca45645971a) to use these models.

| Environment Variable | Description                                  | Where to Find                                      |
| -------------------- | -------------------------------------------- | -------------------------------------------------- |
| `MISTRAL_API_KEY`    | The Mistral API key from Mistral API Console | [Mistral API Console](https://console.mistral.ai/user/api-keys/) |
